{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1d31f54",
   "metadata": {},
   "source": [
    "# Notebook for preprocessing Wikipedia (English) dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21eb8ed4",
   "metadata": {},
   "source": [
    "### Initilizing phonemizer and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6ca5ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "config_path = \"Configs/config.yml\" # you can change it to anything else\n",
    "config = yaml.safe_load(open(config_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b52b79ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from phonemize import phonemize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b363b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import phonemizer\n",
    "global_phonemizer = phonemizer.backend.EspeakBackend(language='en-us', preserve_punctuation=True,  with_stress=True, language_switch=\"remove-flags\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d58c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(config['dataset_params']['tokenizer']) # you can use any other tokenizers if you want to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b71dd813",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_directory = \"/mnt/data/wiki_phoneme\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb25417",
   "metadata": {},
   "source": [
    "### Process dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25e5ae16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"wikimedia/wikipedia\", \"20231101.hi\")['train'] # you can use other version of this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7a15189",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92a578d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "num_shards = 50000\n",
    "\n",
    "def process_shard(i):\n",
    "    directory = root_directory + \"/shard_\" + str(i)\n",
    "    if os.path.exists(directory):\n",
    "        print(\"Shard %d already exists!\" % i)\n",
    "        return\n",
    "    print('Processing shard %d ...' % i)\n",
    "    shard = dataset.shard(num_shards=num_shards, index=i)\n",
    "    processed_dataset = shard.map(lambda t: phonemize(t['text'], global_phonemizer, tokenizer), remove_columns=['text'])\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    processed_dataset.save_to_disk(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d73caf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_shards_parallel(num_processes=96):\n",
    "    \"\"\"\n",
    "    Process all shards using multiprocessing\n",
    "    \"\"\"\n",
    "    print(f\"Starting processing with {num_processes} processes\")\n",
    "    \n",
    "    with Pool(processes=num_processes) as pool:\n",
    "        # Process shards with progress bar\n",
    "        for _ in tqdm(\n",
    "            pool.imap_unordered(process_shard, range(num_shards)),\n",
    "            total=num_shards,\n",
    "            desc=\"Processing shards\"\n",
    "        ):\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04261364",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "process_shards_parallel(num_processes=96)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78caee6",
   "metadata": {},
   "source": [
    "### Collect all shards to form the processed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0568da38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk, concatenate_datasets\n",
    "import os \n",
    "\n",
    "output = [dI for dI in os.listdir(root_directory) if os.path.isdir(os.path.join(root_directory,dI))]\n",
    "datasets = []\n",
    "for o in output:\n",
    "    directory = root_directory + \"/\" + o\n",
    "    try:\n",
    "        shard = load_from_disk(directory)\n",
    "        datasets.append(shard)\n",
    "        # print(\"%s loaded\" % o)\n",
    "    except:\n",
    "        print(\"Continued\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1547f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = concatenate_datasets(datasets)\n",
    "# dataset.save_to_disk(config['data_folder'])\n",
    "print('Dataset saved to %s' % config['data_folder'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce557e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.push_to_hub(\n",
    "    \"wasimmadha/plbert-dataset-hindi\",  # replace with your desired repository name\n",
    "    private=True,  # set to True if you want it private\n",
    "    token=\"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\"  # token will be used from the previous login\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce886d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the dataset size\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf6f6f6",
   "metadata": {},
   "source": [
    "### Remove unneccessary tokens from the pre-trained tokenizer\n",
    "The pre-trained tokenizer contains a lot of tokens that are not used in our dataset, so we need to remove these tokens. We also want to predict the word in lower cases because cases do not matter that much for TTS. Pruning the tokenizer is much faster than training a new tokenizer from scratch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73dda2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "# from simple_loader import FilePathDataset, build_dataloader\n",
    "    \n",
    "# # Load dataset from hub\n",
    "# dataset = load_dataset(\"wasimmadha/plbert-dataset\", token=\"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\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cec407",
   "metadata": {},
   "outputs": [],
   "source": [
    "from simple_loader import FilePathDataset, build_dataloader\n",
    "\n",
    "file_data1 = FilePathDataset(dataset)\n",
    "loader = build_dataloader(file_data, num_workers=32, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "57aa9bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = build_dataloader(file_data1, num_workers=32, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5f4507",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, batch in loader:\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62477c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_data[0]['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce937a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_data1[0]['phonemes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "47c1261b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"/home/ubuntu/PL-BERT/multilingual-pl-bert/token_maps.pkl\", 'rb') as handle:\n",
    "    token_maps = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e96a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_maps[50100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca55d3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfae8b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode(text=\"कन्हैयालाल सेठिया\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0b7504eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_token = config['dataset_params']['word_separator']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcb44a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all unique tokens in the entire dataset\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "unique_index = [special_token]\n",
    "for _, batch in enumerate(tqdm(loader)):\n",
    "    unique_index.extend(batch)\n",
    "    unique_index = list(unique_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1445662d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get each token's lower case\n",
    "\n",
    "lower_tokens = []\n",
    "for t in tqdm(unique_index):\n",
    "    word = tokenizer.decode(t)\n",
    "    if word.lower() != word:\n",
    "        t = tokenizer.encode(word.lower())\n",
    "        lower_tokens.append(t)\n",
    "    else:\n",
    "        lower_tokens.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2dea92",
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_tokens = (list(set(lower_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf6f61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a76cda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# redo the mapping for lower number of tokens\n",
    "\n",
    "token_maps = {}\n",
    "for t in tqdm(unique_index):\n",
    "    word = tokenizer.decode([t])\n",
    "    word = word.lower()\n",
    "    new_t = tokenizer.encode([word.lower()])[0]\n",
    "    token_maps[t] = {'word': word, 'token': lower_tokens.index(new_t)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c94be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(config['dataset_params']['token_maps'], 'wb') as handle:\n",
    "    pickle.dump(token_maps, handle)\n",
    "print('Token mapper saved to %s' % config['dataset_params']['token_maps'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9e968e",
   "metadata": {},
   "source": [
    "### Test the dataset with dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9025e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader import build_dataloader\n",
    "\n",
    "train_loader = build_dataloader(dataset, batch_size=32, num_workers=0, dataset_config=config['dataset_params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "70874215",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, (words, labels, phonemes, input_lengths, masked_indices) = next(enumerate(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61fee30",
   "metadata": {},
   "outputs": [],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d8b23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7d3b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "phonemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ae46d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3448dbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86bdd07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
