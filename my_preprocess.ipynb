{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/PL-BERT/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from simple_loader import FilePathDataset, build_dataloader\n",
    "from datasets import load_from_disk\n",
    "import yaml, pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/PL-BERT/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "multi_lingual_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'toren'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_lingual_tokenizer.decode([72617])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_ids = []\n",
    "\n",
    "# for token in token_maps:\n",
    "#     index = token_maps[token]['token']\n",
    "\n",
    "#     unique_ids.append(index)\n",
    "\n",
    "# unique_ids = list(set(unique_ids))\n",
    "\n",
    "# # Sort the list\n",
    "# unique_ids.sort()\n",
    "\n",
    "# is_consecutive = unique_ids == list(range(min(unique_ids), max(unique_ids) + 1))\n",
    "\n",
    "# # Print results\n",
    "# print(f\"unique_ids are {'consecutive' if is_consecutive else 'not consecutive'}\")\n",
    "# print(\"Missing numbers (if any):\", set(range(min(unique_ids), max(unique_ids) + 1)) - set(unique_ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'unique_ids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mmax\u001b[39m(\u001b[43munique_ids\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'unique_ids' is not defined"
     ]
    }
   ],
   "source": [
    "# max(unique_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from itertools import chain\n",
    "\n",
    "# keys = set(token_maps.keys())  # Convert to set for faster lookup\n",
    "# for ds in hindi_dataset:\n",
    "#     flattened_list = list(chain.from_iterable(ds['input_ids']))\n",
    "#     missing_elements = set(flattened_list) - keys\n",
    "    \n",
    "#     if missing_elements:\n",
    "#         print(\"Elements in flattened_list but not in token_maps keys:\", missing_elements)\n",
    "#         print(\"Count of missing elements:\", len(missing_elements))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from itertools import chain\n",
    "\n",
    "# keys = set(token_maps.keys())  # Convert to set for faster lookup\n",
    "\n",
    "# for ds in hindi_dataset:\n",
    "#     flattened_list = list(chain.from_iterable(ds['input_ids']))\n",
    "#     missing_elements = set(flattened_list) - keys\n",
    "    \n",
    "#     if missing_elements:\n",
    "#         print(\"Count of missing elements:\", len(missing_elements))\n",
    "#         print(\"Elements in flattened_list but not in token_maps keys:\", missing_elements)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "hindi_dataset = load_from_disk(\"/home/PL-BERT/data/multilingual-pl-bert/hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = FilePathDataset(hindi_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/PL-BERT/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "loader = build_dataloader(hindi_dataset, num_workers=32, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"/home/PL-BERT/new_token_maps.pkl\", 'rb') as handle:\n",
    "    token_maps = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'word': '##מל', 'token': 94551}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_maps[28631]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = \"/home/ubuntu/PL-BERT/multilingual-pl-bert/config.yml\" # you can change it to anything else\n",
    "config = yaml.safe_load(open(config_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'log_dir': 'Checkpoint_all_phonemes',\n",
       " 'mixed_precision': 'fp16',\n",
       " 'data_folder': 'wikipedia_20220301.en.processed',\n",
       " 'batch_size': 32,\n",
       " 'save_interval': 20000,\n",
       " 'log_interval': 10,\n",
       " 'num_process': 1,\n",
       " 'num_steps': 2000000,\n",
       " 'dataset_params': {'tokenizer': 'bert-base-multilingual-cased',\n",
       "  'token_separator': ' ',\n",
       "  'token_mask': 'M',\n",
       "  'word_separator': 102,\n",
       "  'token_maps': 'token_maps.pkl',\n",
       "  'max_mel_length': 512,\n",
       "  'word_mask_prob': 0.15,\n",
       "  'phoneme_mask_prob': 0.1,\n",
       "  'replace_prob': 0.2},\n",
       " 'model_params': {'vocab_size': 178,\n",
       "  'hidden_size': 768,\n",
       "  'num_attention_heads': 12,\n",
       "  'intermediate_size': 2048,\n",
       "  'max_position_embeddings': 512,\n",
       "  'num_hidden_layers': 12,\n",
       "  'dropout': 0.1}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_token = config['dataset_params']['word_separator']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 269/269 [00:21<00:00, 12.47it/s]\n"
     ]
    }
   ],
   "source": [
    "# get all unique tokens in the entire dataset\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "unique_index = [special_token]\n",
    "for _, batch in enumerate(tqdm(loader)):\n",
    "    flattened_batch = list(chain.from_iterable(batch))\n",
    "    flattened_batch = list(set(flattened_batch))\n",
    "    # for ids in batch:\n",
    "    unique_index.extend(flattened_batch)\n",
    "    unique_index = list(set(unique_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All numbers are consecutive: False\n",
      "Max:  94550\n"
     ]
    }
   ],
   "source": [
    "unique_ids = []\n",
    "\n",
    "for token in token_maps:\n",
    "    index = token_maps[token]['token']\n",
    "\n",
    "    unique_ids.append(index)\n",
    "\n",
    "unique_ids = list(set(unique_ids))\n",
    "\n",
    "# Sort the list\n",
    "unique_ids.sort()\n",
    "\n",
    "# Check if all elements are consecutive\n",
    "are_consecutive = all(unique_ids[i] + 1 == unique_ids[i + 1] for i in range(len(unique_ids) - 1))\n",
    "\n",
    "# Output result\n",
    "print(\"All numbers are consecutive:\", are_consecutive)\n",
    "print(\"Max: \", max(unique_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34616/34616 [00:04<00:00, 7117.87it/s] \n"
     ]
    }
   ],
   "source": [
    "# redo the mapping for lower number of tokens\n",
    "\n",
    "for t in tqdm(unique_index):\n",
    "    if t in token_maps.keys():\n",
    "        continue\n",
    "\n",
    "    word = multi_lingual_tokenizer.decode([t])\n",
    "    token_maps[t] = {'word': word, 'token': unique_index.index(t) + max(unique_ids)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79818"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(token_maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"new_token_maps.pkl\", 'wb') as handle:\n",
    "    pickle.dump(token_maps, handle)\n",
    "# print('Token mapper saved to %s' % config['dataset_params']['token_maps'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens_not_available = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for t in tqdm(unique_index):\n",
    "#     try: \n",
    "#         token_maps[t]\n",
    "#         pass\n",
    "#     except:\n",
    "#         print(multi_lingual_tokenizer.decode([t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mydataloader import build_dataloader, FilePathDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "177\n"
     ]
    }
   ],
   "source": [
    "myds = FilePathDataset(hindi_dataset, tokenizer=multi_lingual_tokenizer, token_maps=\"new_token_maps.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "177\n"
     ]
    }
   ],
   "source": [
    "loader = build_dataloader(hindi_dataset,batch_size=1\n",
    ",dataset_config={\n",
    "    \"tokenizer\": multi_lingual_tokenizer, \n",
    "    \"token_maps\":\"new_token_maps.pkl\",\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in loader:\n",
    "    words, labels, phonemes, input_lengths, masked_indices = batch\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 16,  46, 156,  57, 158,  16,  50, 156, 138,  90,  43, 158, 125,  16,\n",
       "          62, 156,  51, 158,  56,  16,  83,  47,  57, 158, 136,  16,  44, 156,\n",
       "         138,  56,  51,  16,  50, 156, 102,  56,  46,  51,  16,  44, 162, 156,\n",
       "          43, 158, 130,  43, 158,  16,  53,  51,  16,  58, 162, 156, 102,  54,\n",
       "          55,  83,  16,  50,  86, 158,  16,  58, 156,  63, 158,  60,  56,  65,\n",
       "         102, 125, 156,  43, 158,  55,  16,  61,  83, 112,  53, 131, 156,  47,\n",
       "         158,  58,  16,  45,  83, 125, 156, 102,  62, 125,  83,  16,  55, 156,\n",
       "         135,  53, 162,  52,  83,  16,  53, 157,  83,  54,  43, 158,  53, 156,\n",
       "          43, 158, 125,  16, 157, 138,  44, 162, 102, 130, 156,  47, 158,  53,\n",
       "          16,  44, 156, 138,  45, 158,  83,  56,  16,  54, 156,  43, 158, 125,\n",
       "          43, 158,  16,  46, 156, 138,  62, 158,  43, 158,  16,  45, 156, 138,\n",
       "         112,  53,  51,  16,  58, 156,  43,  37,  80,  47, 158,  16,  37,  16,\n",
       "          61, 156, 138, 114,  90,  83, 157,  86,  52,  16,  43, 158,  46, 156,\n",
       "         102,  62, 158,  52,  83,  16,  54, 156, 138,  53, 162, 102,  52, 157,\n",
       "          43, 158,  16,  37,  16,  61, 156,  63, 158,  60,  52,  43, 158,  16,\n",
       "          52, 157,  83, 131,  58, 156,  43, 158,  54,  16, 131, 156, 138,  60,\n",
       "          55,  43, 158,  16,  46, 156, 138,  52,  43, 158,  16, 131, 156, 138,\n",
       "         112,  53,  83, 125,  16,  58, 156,  43,  37,  80,  47, 158,  16,  37,\n",
       "          16,  50, 156, 138, 125,  51,  16, 157, 138,  53, 162, 102,  54, 156,\n",
       "          47, 158,  56,  46, 125,  83,  16,  55, 156, 102, 131, 125,  43, 158,\n",
       "          16,  37,  16,  58, 135,  90, 156,  43, 158, 125,  51,  16, 125,  43,\n",
       "         158,  92, 156,  47, 158, 131, 136,  83, 125, 157,  51,  16,  29,  29,\n",
       "          29,  29,  29,  29,  29,  29,  16,  54, 156, 138,  53, 162, 102,  52,\n",
       "         157,  43, 158,  16, 125,  43, 158,  90, 156,  47, 158,  56,  46, 125,\n",
       "          83,  16,  92, 156, 135,  58,  62,  43, 158,  16, 157, 138,  55, 102,\n",
       "          62, 156,  43, 158,  44, 162,  16,  44, 156, 138,  45, 158,  83,  56,\n",
       "          16,  37,  16,  29,  29,  29,  29,  29,  29,  29,  16,  53, 156, 138,\n",
       "          50,  83,  56, 157,  47, 158,  16, 136, 156,  43, 158,  54,  43, 158,\n",
       "          16,  46, 156, 138,  54,  16,  61,  83, 112,  92, 156,  51, 158,  62,\n",
       "          16, 125, 156,  57, 158,  45,  83,  53,  16,  29,  29,  29,  29,  29,\n",
       "          29,  29,  16,  58, 157,  83, 125, 102, 113, 156,  43, 158,  55,  16,\n",
       "          44, 156,  76, 158,  53,  61,  16, 156,  76,  58, 162, 102,  61,  16,\n",
       "          61,  83,  55, 156,  51, 158,  53, 131,  43, 158,  52,  16,  56,  43,\n",
       "         158,  55, 156,  43,  37,  53,  83,  56,  16,  29,  29,  29,  16,  58,\n",
       "         157, 135, 125,  61,  53, 156,  43, 158, 125,  16,  44, 156,  43, 158,\n",
       "          50,  83, 125, 157,  51,  16,  53, 156, 138,  60,   4, 102,  52, 157,\n",
       "          43,  37,  16,  46, 156,  57, 158,  16]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phonemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1424,  6697,  6697,  6697,  6697,  6697,  6697,  6697,  6697,  6697,\n",
       "           6697,  6697,  6697,  6697,  6697,  6697,  6697,  6697,  6697,  1424,\n",
       "          61857, 61857, 61857, 61857, 61857,  1424, 90475, 90475, 90475, 90475,\n",
       "          90475,  1424, 66712, 66712, 66712, 66712, 66712, 66712,  1424, 62954,\n",
       "          62954, 62954, 62954, 62954, 62954, 62954, 62954,  1424, 62085, 62085,\n",
       "           1424, 76541, 76541, 76541, 76541, 76541, 76541, 76541,  1424, 61890,\n",
       "          61890, 61890,  1424, 60380, 60380, 60380, 60380, 60380, 60380, 60380,\n",
       "          60380, 60380, 60380, 60380, 60380, 60380,  1424, 71302, 71302, 71302,\n",
       "          73013, 73013, 73013, 62641, 62641, 66551,  1424,   368,  7635,  7635,\n",
       "          77870, 77870, 77870, 77870, 77870,  1424, 72408, 72408, 72408, 72408,\n",
       "          72408, 72408, 72408,  1424, 93676, 93676, 93676, 93676, 93676, 93676,\n",
       "          93676, 93676, 93676, 93676, 93676,  1424,   363,   363, 34433,  8522,\n",
       "           8522, 23608, 23608, 80329, 80329, 80329,  1424,   373, 11042, 11042,\n",
       "          13069, 90228, 90228, 90228,  1424, 68041, 68041, 68041, 68041, 72454,\n",
       "          72454, 72454,  1424, 60345, 90805, 90805, 90805, 90805, 90805, 90805,\n",
       "           1424,   368,   368, 90085, 90085, 62154, 62154,  1424, 60348, 62375,\n",
       "          62375, 71847, 71847, 71847, 62641,  1424,    13,  1424, 71302, 71302,\n",
       "          71302, 71302, 66122, 66122, 66122, 64134, 64134,  1424, 66585, 66585,\n",
       "          66585, 66585, 90275, 90275, 90275, 90275, 90275,  1424, 60356, 60356,\n",
       "          71042, 71042, 68167, 68167, 68167, 68167, 68167, 68167,  1424,    13,\n",
       "           1424,   376, 77453, 77453, 77453, 66053, 66053, 66053, 66053,  1424,\n",
       "          60353, 60353, 13855, 13855, 76416, 76416, 76416,  7746,  7746,  1424,\n",
       "            375, 88718, 88718, 88718, 88718, 88718, 88718,  1424, 60345, 60345,\n",
       "          64917, 64917, 64917, 64917,  1424,   375,   375, 90085, 90085, 90085,\n",
       "           7635,  7635,  1424, 60348, 62375, 62375, 71847, 71847, 71847, 62641,\n",
       "           1424,    13,  1424, 88894, 88894, 88894, 62154, 62154,  1424,   363,\n",
       "          71042, 71042, 79902, 79902, 79902, 86394, 86394, 86394, 86394, 11854,\n",
       "          11854, 11854,  1424, 60352, 83750, 83750, 43454, 43454, 43454, 43454,\n",
       "           1424,    13,  1424, 60348,  9289,  9289, 87748, 87748, 87748, 87748,\n",
       "          62154,  1424, 60354, 86189, 86189, 86189, 86622, 86622, 86622, 86622,\n",
       "          86622, 86622, 86622, 86622, 62154,  1424,   376, 73059, 73059, 73059,\n",
       "          73059, 11153, 11153,  7746,  1424, 60356, 60356, 71042, 71042, 68167,\n",
       "          68167, 68167, 68167, 68167, 68167,  1424, 60354, 77328, 77328, 77328,\n",
       "          75629, 75629, 82228, 82228, 82228, 11854, 11854,  1424,   367,  9289,\n",
       "          76973, 76973, 76973, 76973, 62375,  1424,   363,   363,  9153, 84146,\n",
       "          84146, 84146, 84146, 84146, 34433, 34433,  1424,   373, 11042, 11042,\n",
       "          13069, 90228, 90228, 90228,  1424,    13,  1424, 88663, 88663, 88663,\n",
       "          88663, 88663, 88663, 88663,  1424, 60328, 60328, 76899, 76899, 76899,\n",
       "          76899, 76899, 62641, 62641,  1424, 82812, 82812, 82812, 82812, 82812,\n",
       "          82812, 82812,  1424, 60345, 60345,  7746,  7746,  1424, 77357, 77357,\n",
       "          77357, 77357, 77357, 77357, 77357, 77357,  1424, 60354, 60354,  9077,\n",
       "           9077, 11042,  8041,  8041,  1424, 83777, 83777, 83777, 83777, 83777,\n",
       "          83777, 83777,  1424, 93177, 93177, 93177, 93177, 93177, 93177, 93177,\n",
       "          93177, 93177, 93177,  1424,   373, 43835, 92100, 92100, 92100, 92100,\n",
       "           1424, 76028, 76028, 76028, 71838, 71838, 71838,  1424,   376, 75831,\n",
       "          75831, 75831, 79643, 79643, 79643, 79643, 79643, 82079, 82079,  1424,\n",
       "          68296, 68296, 68296, 68296, 92532, 92532, 92532, 92532, 92532,  7574,\n",
       "           1424, 62197, 62197, 62197,  1424, 70205, 70205, 70205, 70205, 70205,\n",
       "          70205, 70205, 70205, 70205, 70205,  1424, 90660, 90660, 90660, 90660,\n",
       "          90660, 90660, 90660, 62154, 62154,  1424, 60328, 76994, 76994, 76994,\n",
       "          68167, 68167, 68167, 68167, 68167, 71301,  1424,  6697,  6697,  6697,\n",
       "           6697,  6697]]),\n",
       " tensor([[ 16,  46, 156,  57, 158,  16,  50, 156, 138,  90,  43, 158, 125,  16,\n",
       "           62, 156,  51, 158,  56,  16,  83,  47,  57, 158, 136,  16,  44, 156,\n",
       "          138,  56,  51,  16,  50, 156, 102,  56,  46,  51,  16,  44, 162, 156,\n",
       "           43, 158, 130,  43, 158,  16,  53,  51,  16,  58, 162, 156, 102,  54,\n",
       "           55,  83,  16,  50,  86, 158,  16,  58, 156,  63, 158,  60,  56,  65,\n",
       "          102, 125, 156,  43, 158,  55,  16,  61,  83, 112,  53, 131, 156,  47,\n",
       "          158,  58,  16,  45,  83, 125, 156, 102,  62, 125,  83,  16,  55, 156,\n",
       "          135,  53, 162,  52,  83,  16,  53, 157,  83,  54,  43, 158,  53, 156,\n",
       "           43, 158, 125,  16, 157, 138,  44, 162, 102, 130, 156,  47, 158,  53,\n",
       "           16,  44, 156, 138,  45, 158,  83,  56,  16,  54, 156,  43, 158, 125,\n",
       "           43, 158,  16,  46, 156, 138,  62, 158,  43, 158,  16,  45, 156, 138,\n",
       "          112,  53,  51,  16,  58, 156,  43,  37,  80,  47, 158,  16,  37,  16,\n",
       "           61, 156, 138, 114,  90,  83, 157,  86,  52,  16,  43, 158,  46, 156,\n",
       "          102,  62, 158,  52,  83,  16,  54, 156, 138,  53, 162, 102,  52, 157,\n",
       "           43, 158,  16,  37,  16,  61, 156,  63, 158,  60,  52,  43, 158,  16,\n",
       "           52, 157,  83, 131,  58, 156,  43, 158,  54,  16, 131, 156, 138,  60,\n",
       "           55,  43, 158,  16,  46, 156, 138,  52,  43, 158,  16, 131, 156, 138,\n",
       "          112,  53,  83, 125,  16,  58, 156,  43,  37,  80,  47, 158,  16,  37,\n",
       "           16,  50, 156, 138, 125,  51,  16, 157, 138,  53, 162, 102,  54, 156,\n",
       "           47, 158,  56,  46, 125,  83,  16,  55, 156, 102, 131, 125,  43, 158,\n",
       "           16,  37,  16,  58, 135,  90, 156,  43, 158, 125,  51,  16, 125,  43,\n",
       "          158,  92, 156,  47, 158, 131, 136,  83, 125, 157,  51,  16,  29,  29,\n",
       "           29,  29,  29,  29,  29,  29,  16,  54, 156, 138,  53, 162, 102,  52,\n",
       "          157,  43, 158,  16, 125,  43, 158,  90, 156,  47, 158,  56,  46, 125,\n",
       "           83,  16,  92, 156, 135,  58,  62,  43, 158,  16, 157, 138,  55, 102,\n",
       "           62, 156,  43, 158,  44, 162,  16,  44, 156, 138,  45, 158,  83,  56,\n",
       "           16,  37,  16,  29,  29,  29,  29,  29,  29,  29,  16,  53, 156, 138,\n",
       "           50,  83,  56, 157,  47, 158,  16, 136, 156,  43, 158,  54,  43, 158,\n",
       "           16,  46, 156, 138,  54,  16,  61,  83, 112,  92, 156,  51, 158,  62,\n",
       "           16, 125, 156,  57, 158,  45,  83,  53,  16,  29,  29,  29,  29,  29,\n",
       "           29,  29,  16,  58, 157,  83, 125, 102, 113, 156,  43, 158,  55,  16,\n",
       "           44, 156,  76, 158,  53,  61,  16, 156,  76,  58, 162, 102,  61,  16,\n",
       "           61,  83,  55, 156,  51, 158,  53, 131,  43, 158,  52,  16,  56,  43,\n",
       "          158,  55, 156,  43,  37,  53,  83,  56,  16,  29,  29,  29,  16,  58,\n",
       "          157, 135, 125,  61,  53, 156,  43, 158, 125,  16,  44, 156,  43, 158,\n",
       "           50,  83, 125, 157,  51,  16,  53, 156, 138,  60,   4, 102,  52, 157,\n",
       "           43,  37,  16,  46, 156,  57, 158,  16]]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words, phonemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
