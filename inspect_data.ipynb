{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from phonemize import phonemize\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import phonemizer\n",
    "global_phonemizer = phonemizer.backend.EspeakBackend(language='en-us', preserve_punctuation=True,  with_stress=True, language_switch=\"remove-flags\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = \"Configs/config.yml\" # you can change it to anything else\n",
    "config = yaml.safe_load(open(config_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TransfoXLTokenizer\n",
    "\n",
    "english_tokenizer =  TransfoXLTokenizer.from_pretrained(\"transfo-xl-wt103\") # you can use any other tokenizers if you want to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_directory = \"/mnt/data/wiki_phoneme\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk, concatenate_datasets\n",
    "import os \n",
    "\n",
    "output = [dI for dI in os.listdir(root_directory) if os.path.isdir(os.path.join(root_directory,dI))]\n",
    "datasets = []\n",
    "for o in output:\n",
    "    directory = root_directory + \"/\" + o\n",
    "    try:\n",
    "        shard = load_from_disk(directory)\n",
    "        datasets.append(shard)\n",
    "        # print(\"%s loaded\" % o)\n",
    "    except:\n",
    "        print(\"Continued\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_dataset = concatenate_datasets(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simple_loader import FilePathDataset, build_dataloader\n",
    "\n",
    "english_file_data1 = FilePathDataset(english_dataset)\n",
    "en_simple_loader = build_dataloader(english_file_data1, num_workers=32, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in en_simple_loader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# english_dataset[0]['phonemes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader import FilePathDataset, build_dataloader\n",
    "\n",
    "english_file_data1 = FilePathDataset(dataset=english_dataset)\n",
    "en_loader = build_dataloader(english_dataset, num_workers=32, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phonemes, words, labels, masked_index = english_file_data1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.token_maps[english_dataset[0]['input_ids'][3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_tokenizer.decode(english_dataset[0]['input_ids'][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in en_loader:\n",
    "    words, labels, phonemes, input_lengths, masked_indices = batch\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(english_dataset[0]['phonemes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(english_dataset[0]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "hindi_dataset = load_from_disk(\"/home/ubuntu/PL-BERT/data/multilingual-pl-bert/hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coding: utf-8\n",
    "\n",
    "import os\n",
    "import os.path as osp\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import string\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from text_utils import TextCleaner\n",
    "from utils import align_subtokens_to_phonemes\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "np.random.seed(1)\n",
    "random.seed(1)\n",
    "\n",
    "class FilePathDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset,\n",
    "                 token_maps=\"token_maps.pkl\",\n",
    "                 tokenizer=\"transfo-xl-wt103\",\n",
    "                 word_separator=3039, \n",
    "                 token_separator=\" \", \n",
    "                 token_mask=\"M\", \n",
    "                 max_mel_length=512,\n",
    "                 word_mask_prob=0.15,\n",
    "                 phoneme_mask_prob=0.1,\n",
    "                 replace_prob=0.2):\n",
    "        \n",
    "        self.data = dataset\n",
    "        self.max_mel_length = max_mel_length\n",
    "        self.word_mask_prob = word_mask_prob\n",
    "        self.phoneme_mask_prob = phoneme_mask_prob\n",
    "        self.replace_prob = replace_prob\n",
    "        self.text_cleaner = TextCleaner()\n",
    "        \n",
    "        self.word_separator = word_separator\n",
    "        self.token_separator = token_separator\n",
    "        self.token_mask = token_mask\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        with open(token_maps, 'rb') as handle:\n",
    "            self.token_maps = pickle.load(handle)     \n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        phonemes = self.data[idx]['phonemes']\n",
    "        input_ids = self.data[idx]['input_ids']\n",
    "\n",
    "        words = []\n",
    "        labels = \"\"\n",
    "        phoneme = \"\"\n",
    "        print(phonemes)\n",
    "\n",
    "        phoneme_list = ''.join(phonemes)\n",
    "        masked_index = []\n",
    "        for z in zip(phonemes, input_ids):\n",
    "            z = list(z)\n",
    "            \n",
    "            if len(z[1]) == 1:\n",
    "                words.extend([z[1][0]] * len(z[0]))\n",
    "            else:\n",
    "                subtokens = [self.tokenizer.decode(id) for id in z[1]]\n",
    "                mapping = align_subtokens_to_phonemes(subtokens=subtokens, ipa_phonemes=z[0], token_ids=z[1])\n",
    "                phonemes_idx = 0\n",
    "                for subtoken, singl_phoneme, token_id in mapping:\n",
    "                    assert z[0][phonemes_idx:phonemes_idx+len(singl_phoneme)] == singl_phoneme\n",
    "                    phonemes_idx += len(singl_phoneme)\n",
    "                    words.extend([token_id] * len(singl_phoneme))\n",
    "\n",
    "            words.append(self.word_separator)\n",
    "            labels += z[0] + \" \"\n",
    "\n",
    "            if np.random.rand() < self.word_mask_prob:\n",
    "                if np.random.rand() < self.replace_prob:\n",
    "                    if np.random.rand() < (self.phoneme_mask_prob / self.replace_prob): \n",
    "                        phoneme += ''.join([phoneme_list[np.random.randint(0, len(phoneme_list))] for _ in range(len(z[0]))])  # randomized\n",
    "                    else:\n",
    "                        phoneme += z[0]\n",
    "                else:\n",
    "                    phoneme += self.token_mask * len(z[0]) # masked\n",
    "                    \n",
    "                masked_index.extend((np.arange(len(phoneme) - len(z[0]), len(phoneme))).tolist())\n",
    "            else:\n",
    "                phoneme += z[0] \n",
    "\n",
    "            phoneme += self.token_separator\n",
    "\n",
    "        print(\"After Loop: \", phoneme)\n",
    "        mel_length = len(phoneme)\n",
    "        masked_idx = np.array(masked_index)\n",
    "        masked_index = []\n",
    "        if mel_length > self.max_mel_length:\n",
    "            random_start = np.random.randint(0, mel_length - self.max_mel_length)\n",
    "            phoneme = phoneme[random_start:random_start + self.max_mel_length]\n",
    "            words = words[random_start:random_start + self.max_mel_length]\n",
    "            labels = labels[random_start:random_start + self.max_mel_length]\n",
    "            \n",
    "            for m in masked_idx:\n",
    "                if m >= random_start and m < random_start + self.max_mel_length:\n",
    "                    masked_index.append(m - random_start)\n",
    "        else:\n",
    "            masked_index = masked_idx\n",
    "        print(phoneme)\n",
    "        phoneme = self.text_cleaner(phoneme)\n",
    "        print(phoneme)\n",
    "        labels = self.text_cleaner(labels)\n",
    "        words = [self.token_maps[w]['token'] for w in words]\n",
    "        \n",
    "        assert len(phoneme) == len(words)\n",
    "        assert len(phoneme) == len(labels)\n",
    "        \n",
    "        print(phoneme, words, labels)\n",
    "        phonemes = torch.LongTensor(phoneme)\n",
    "        labels = torch.LongTensor(labels)\n",
    "        words = torch.LongTensor(words)\n",
    "        \n",
    "        return phoneme, words, labels, masked_index\n",
    "        \n",
    "class Collater(object):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      adaptive_batch_size (bool): if true, decrease batch size when long data comes.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, return_wave=False):\n",
    "        self.text_pad_index = 0\n",
    "        self.return_wave = return_wave\n",
    "        \n",
    "\n",
    "    def __call__(self, batch):\n",
    "        # batch[0] = wave, mel, text, f0, speakerid\n",
    "        batch_size = len(batch)\n",
    "\n",
    "        # sort by mel length\n",
    "        lengths = [b[1].shape[0] for b in batch]\n",
    "        batch_indexes = np.argsort(lengths)[::-1]\n",
    "        batch = [batch[bid] for bid in batch_indexes]\n",
    "\n",
    "        max_text_length = max([b[1].shape[0] for b in batch])\n",
    "\n",
    "        words = torch.zeros((batch_size, max_text_length)).long()\n",
    "        labels = torch.zeros((batch_size, max_text_length)).long()\n",
    "        phonemes = torch.zeros((batch_size, max_text_length)).long()\n",
    "        input_lengths = []\n",
    "        masked_indices = []\n",
    "        for bid, (phoneme, word, label, masked_index) in enumerate(batch):\n",
    "            \n",
    "            text_size = phoneme.size(0)\n",
    "            words[bid, :text_size] = word\n",
    "            labels[bid, :text_size] = label\n",
    "            phonemes[bid, :text_size] = phoneme\n",
    "            input_lengths.append(text_size)\n",
    "            masked_indices.append(masked_index)\n",
    "\n",
    "        return words, labels, phonemes, input_lengths, masked_indices\n",
    "\n",
    "\n",
    "def build_dataloader(df,\n",
    "                     validation=False,\n",
    "                     batch_size=4,\n",
    "                     num_workers=1,\n",
    "                     device='cpu',\n",
    "                     collate_config={},\n",
    "                     dataset_config={}):\n",
    "\n",
    "    dataset = FilePathDataset(df, **dataset_config)\n",
    "    collate_fn = Collater(**collate_config)\n",
    "    data_loader = DataLoader(dataset,\n",
    "                             batch_size=batch_size,\n",
    "                             shuffle=(not validation),\n",
    "                             num_workers=num_workers,\n",
    "                             drop_last=(not validation),\n",
    "                             collate_fn=collate_fn,\n",
    "                             pin_memory=(device != 'cpu'))\n",
    "\n",
    "    return data_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "multi_lingual_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = FilePathDataset(hindi_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_subtokens_to_phonemes(subtokens, ipa_phonemes, token_ids):\n",
    "    \"\"\"\n",
    "    Align subtokens to IPA phonemes along with token IDs based on length and sound structure.\n",
    "    \n",
    "    Parameters:\n",
    "    subtokens (list of str): List of subtokens for a word.\n",
    "    ipa_phonemes (str): IPA phoneme string for the word.\n",
    "    token_ids (list of int): List of token IDs for each subtoken.\n",
    "    \n",
    "    Returns:\n",
    "    list of tuple: Each tuple contains a subtoken, its aligned IPA phoneme, and its token ID.\n",
    "    \"\"\"\n",
    "    # Check if subtokens and token IDs are of equal length\n",
    "    if len(subtokens) != len(token_ids):\n",
    "        raise ValueError(\"Subtokens and token IDs must be of the same length.\")\n",
    "    \n",
    "    # Step 1: Calculate approximate length of IPA phonemes for each subtoken\n",
    "    ipa_length = len(ipa_phonemes)\n",
    "    subtoken_lengths = [len(st.replace('##', '')) for st in subtokens]\n",
    "    total_length = sum(subtoken_lengths)\n",
    "    \n",
    "    # Determine split points in the IPA phonemes based on subtoken lengths\n",
    "    split_points = []\n",
    "    accumulated = 0\n",
    "    for length in subtoken_lengths:\n",
    "        portion = (length / total_length) * ipa_length\n",
    "        accumulated += portion\n",
    "        split_points.append(int(round(accumulated)))  # Round to nearest index\n",
    "    \n",
    "    # Step 2: Split the IPA phoneme string at the determined points\n",
    "    aligned_phonemes = []\n",
    "    start = 0\n",
    "    for end in split_points:\n",
    "        aligned_phonemes.append(ipa_phonemes[start:end])\n",
    "        start = end\n",
    "\n",
    "    # Step 3: Create a mapping of each subtoken to its aligned IPA phoneme and token ID\n",
    "    mapping = list(zip(subtokens, aligned_phonemes, token_ids))\n",
    "    \n",
    "    return mapping\n",
    "\n",
    "# Updated data including token IDs\n",
    "data = [\n",
    "    (['र', '##ी', '##वा'], 'ɾˈiːʋaː', [891, 10914, 28960]),\n",
    "    (['के'], 'keː', [10412]),\n",
    "    (['ज', '##ंग', '##लों'], 'ɟˈʌŋɡəlˌõ', [872, 31222, 51665]),\n",
    "    (['में'], 'mẽː', [10532]),\n",
    "    (['ही'], 'hˈi', [14080]),\n",
    "    (['स', '##फ', '##ेद'], 'səpʰˈeːd', [898, 28863, 82813]),\n",
    "    (['ब', '##ा', '##घ'], 'bˈaːɡʰ', [887, 11208, 55759]),\n",
    "    (['की'], 'ki', [10826]),\n",
    "    (['न', '##स', '##्ल'], 'nˈʌslə', [884, 13432, 50101]),\n",
    "    (['प', '##ाई'], 'pˈaːi', [885, 30472]),\n",
    "    \n",
    "    # New test data\n",
    "    (['हैं'], 'hɛ̃', [11716]),\n",
    "    (['।'], 'pˈuːrnwɪɾˈaːm', [920]),\n",
    "    (['जिले'], 'ɟˈɪleː', [32291]),\n",
    "    (['की'], 'ki', [10826]),\n",
    "    (['प्रमुख'], 'pɾˈʌmʊkʰ', [29218]),\n",
    "    (['उ', '##प', '##ज'], 'ˈʊpəɟ', [855, 18187, 17413]),\n",
    "    (['ध', '##ान'], 'dʰˈaːn', [883, 21202]),\n",
    "    (['है'], 'hɛː', [10569]),\n",
    "    (['।'], 'pˈuːrnwɪɾˈaːm', [920]),\n",
    "    (['जिले'], 'ɟˈɪleː', [32291]),\n",
    "    (['के'], 'keː', [10412]),\n",
    "    (['त', '##ाला'], 'tˈaːlaː', [880, 65986]),\n",
    "    (['नामक'], 'nˈaːmək', [56734]),\n",
    "    (['ज', '##ंग', '##ल'], 'ɟˈʌŋɡəl', [872, 31222, 11714])\n",
    "]\n",
    "\n",
    "# Run alignment for each word in the new test data\n",
    "for subtokens, ipa_phonemes, token_ids in data:\n",
    "    mapping = align_subtokens_to_phonemes(subtokens, ipa_phonemes, token_ids)\n",
    "    print(f\"IPA: {ipa_phonemes}, Subtokens: {subtokens}, Token IDs: {token_ids}\")\n",
    "    print(\"Mapping (Subtoken, Phoneme, Token ID):\", mapping)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phonemes = dd.data[0]['phonemes']\n",
    "input_ids = dd.data[0]['input_ids']\n",
    "\n",
    "words = []\n",
    "labels = \"\"\n",
    "phoneme = \"\"\n",
    "\n",
    "phoneme_list = ''.join(phonemes)\n",
    "masked_index = []\n",
    "for z in zip(phonemes, input_ids):\n",
    "    z = list(z)\n",
    "    \n",
    "    if len(z[1]) == 1:\n",
    "        words.extend([z[1][0]] * len(z[0]))\n",
    "    else:\n",
    "        subtokens = [multi_lingual_tokenizer.decode(id) for id in z[1]]\n",
    "        mapping = align_subtokens_to_phonemes(subtokens=subtokens, ipa_phonemes=z[0], token_ids=z[1])\n",
    "        phonemes_idx = 0\n",
    "        for subtoken, singl_phoneme, token_id in mapping:\n",
    "            assert z[0][phonemes_idx:phonemes_idx+len(singl_phoneme)] == singl_phoneme\n",
    "            phonemes_idx += len(singl_phoneme)\n",
    "            words.extend([token_id] * len(singl_phoneme))\n",
    "\n",
    "    words.append(dd.word_separator)\n",
    "    labels += z[0] + \" \"\n",
    "\n",
    "    if np.random.rand() < dd.word_mask_prob:\n",
    "        if np.random.rand() < dd.replace_prob:\n",
    "            if np.random.rand() < (dd.phoneme_mask_prob / dd.replace_prob): \n",
    "                phoneme += ''.join([phoneme_list[np.random.randint(0, len(phoneme_list))] for _ in range(len(z[0]))])  # randomized\n",
    "            else:\n",
    "                phoneme += z[0]\n",
    "        else:\n",
    "            phoneme += dd.token_mask * len(z[0]) # masked\n",
    "            \n",
    "        masked_index.extend((np.arange(len(phoneme) - len(z[0]), len(phoneme))).tolist())\n",
    "    else:\n",
    "        phoneme += z[0] \n",
    "\n",
    "    phoneme += dd.token_separator\n",
    "\n",
    "mel_length = len(phoneme)\n",
    "masked_idx = np.array(masked_index)\n",
    "masked_index = []\n",
    "if mel_length > dd.max_mel_length:\n",
    "    random_start = np.random.randint(0, mel_length - dd.max_mel_length)\n",
    "    phoneme = phoneme[random_start:random_start + dd.max_mel_length]\n",
    "    words = words[random_start:random_start + dd.max_mel_length]\n",
    "    labels = labels[random_start:random_start + dd.max_mel_length]\n",
    "    \n",
    "    for m in masked_idx:\n",
    "        if m >= random_start and m < random_start + dd.max_mel_length:\n",
    "            masked_index.append(m - random_start)\n",
    "else:\n",
    "    masked_index = masked_idx\n",
    "\n",
    "print(phoneme[:10])\n",
    "print(labels[:10])\n",
    "\n",
    "phoneme = dd.text_cleaner(phoneme)\n",
    "labels = dd.text_cleaner(labels)\n",
    "\n",
    "# words = [dd.token_maps[w]['token'] for w in words]\n",
    "\n",
    "# assert len(phoneme) == len(words)\n",
    "# assert len(phoneme) == len(labels)\n",
    "\n",
    "# print(phoneme, words, labels)\n",
    "# phonemes = torch.LongTensor(phoneme)\n",
    "# labels = torch.LongTensor(labels)\n",
    "# words = torch.LongTensor(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in words:\n",
    "    print(dd.token_maps[w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.token_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phoneme[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phoneme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phoneme, words, labels, masked_index = dd[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phoneme[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for z in zip(hindi_dataset[0]['phonemes'], hindi_dataset[0]['input_ids']):\n",
    "#     decoded = [multi_lingual_tokenizer.decode([token]) for token in z[1]]\n",
    "\n",
    "#     if len(z[1]) == 1:\n",
    "#         continue\n",
    "#     # print(multi_lingual_tokenizer.decode(z[1]), \": \", z[0], z[1], decoded, len(z[0]), len(z[1]))\n",
    "#     print(\"Mapping: \", align_subtokens_to_phonemes(subtokens=decoded, ipa_phonemes=z[0], token_ids=z[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi_loader = build_dataloader(hindi_dataset, num_workers=32, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in hi_loader:\n",
    "    words, labels, phonemes, input_lengths, masked_indices = batch\n",
    "\n",
    "    print()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
